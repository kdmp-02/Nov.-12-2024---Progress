{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f8c330d-80a5-4493-b0b4-061865456385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each class folder\n",
    "for label, class_dir in enumerate(os.listdir(dataset_dir)):\n",
    "    class_path = os.path.join(dataset_dir, class_dir)\n",
    "    \n",
    "    # Check if the path is a directory\n",
    "    if os.path.isdir(class_path):\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "            image = image.resize((64, 64))  # Resize to 64x64\n",
    "            image = np.array(image) / 255.0  # Normalize to [0, 1]\n",
    "            data.append(image)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da42b43-0f02-4b4c-9f40-2bc1d617ef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 64, 64, 1)\n",
      "(28, 26)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Path to your extracted dataset\n",
    "dataset_dir = \"C:/Users/naomi/Downloads/asl_alphabet_test\"\n",
    "\n",
    "# Lists to hold data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Process each image file and infer the label from the filename\n",
    "for img_name in os.listdir(dataset_dir):\n",
    "    img_path = os.path.join(dataset_dir, img_name)\n",
    "    \n",
    "    # Ensure only image files are processed\n",
    "    if os.path.isfile(img_path) and img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        # Extract label from filename (e.g., \"A\" from \"A_test.jpg\")\n",
    "        label = ord(img_name[0].upper()) - 65  # Convert 'A' to 0, 'B' to 1, ..., 'Z' to 25\n",
    "        labels.append(label)\n",
    "        \n",
    "        # Load image, convert to grayscale, resize, normalize, and add to data\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        image = image.resize((64, 64))  # Resize to 64x64\n",
    "        image = np.array(image) / 255.0  # Normalize to [0, 1]\n",
    "        data.append(image)\n",
    "\n",
    "# Convert to numpy arrays and reshape\n",
    "data = np.array(data).reshape(-1, 64, 64, 1)  # Reshape to (samples, 64, 64, 1)\n",
    "labels = np.array(labels)\n",
    "labels = to_categorical(labels, num_classes=26)  # Assuming 26 classes (A-Z)\n",
    "\n",
    "# Check the shapes of data and labels\n",
    "print(data.shape)  # Expected: (num_samples, 64, 64, 1)\n",
    "print(labels.shape)  # Expected: (num_samples, 26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56e1fcea-3b13-4594-b495-c9d62d67ed26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 3.2599 - val_accuracy: 0.0000e+00 - val_loss: 3.3773\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.0909 - loss: 3.1847 - val_accuracy: 0.0000e+00 - val_loss: 3.6229\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.0909 - loss: 3.1254 - val_accuracy: 0.0000e+00 - val_loss: 4.0724\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.1364 - loss: 3.1846 - val_accuracy: 0.0000e+00 - val_loss: 4.2960\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.0909 - loss: 3.2375 - val_accuracy: 0.0000e+00 - val_loss: 4.2200\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.1364 - loss: 3.2768 - val_accuracy: 0.0000e+00 - val_loss: 4.0073\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.1364 - loss: 3.1450 - val_accuracy: 0.0000e+00 - val_loss: 3.8227\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.1364 - loss: 3.0935 - val_accuracy: 0.0000e+00 - val_loss: 3.7286\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.0909 - loss: 3.0516 - val_accuracy: 0.0000e+00 - val_loss: 3.7467\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.1364 - loss: 3.0756 - val_accuracy: 0.0000e+00 - val_loss: 3.8145\n",
      "Training data shape: (22, 64, 64, 1)\n",
      "Validation data shape: (6, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(26, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation for training data\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10\n",
    ")\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "# Save the model\n",
    "model.save('sign_language_model.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
